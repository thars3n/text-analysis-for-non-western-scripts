{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1H4_h.txt = 130423 chars\n",
      "1H6_h.txt = 116317 chars\n",
      "2H4_h.txt = 141574 chars\n",
      "2H6_h.txt = 135863 chars\n",
      "3H6_h.txt = 129551 chars\n",
      "Ado_c.txt = 111116 chars\n",
      "Ant_t.txt = 133668 chars\n",
      "AWW_c.txt = 121896 chars\n",
      "AYL_c.txt = 114429 chars\n",
      "Cor_t.txt = 146443 chars\n",
      "Cym_t.txt = 147159 chars\n",
      "Err_c.txt = 76924 chars\n",
      "H5_h.txt = 141819 chars\n",
      "H8_h.txt = 128223 chars\n",
      "Ham_t.txt = 163429 chars\n",
      "JC_t.txt = 104561 chars\n",
      "John_t.txt = 112414 chars\n",
      "Lear_t.txt = 140510 chars\n",
      "LLL_c.txt = 115391 chars\n",
      "Lucrece_x.txt = 86177 chars\n",
      "M4M_c.txt = 116348 chars\n",
      "Mac_t.txt = 91625 chars\n",
      "MerchV_c.txt = 112334 chars\n",
      "MND_c.txt = 88608 chars\n",
      "Oth_t.txt = 141395 chars\n",
      "Pericles_x.txt = 97471 chars\n",
      "PhxTur_x.txt = 2072 chars\n",
      "R2_h.txt = 120934 chars\n",
      "R3_h.txt = 156881 chars\n",
      "Rom_t.txt = 130885 chars\n",
      "Shr_c.txt = 111364 chars\n",
      "Sonnets_x.txt = 97204 chars\n",
      "TGV_c.txt = 91686 chars\n",
      "Tim_t.txt = 98749 chars\n",
      "Tit_t.txt = 109892 chars\n",
      "Tmp_c.txt = 88800 chars\n",
      "TN_c.txt = 104476 chars\n",
      "TNK_x.txt = 127691 chars\n",
      "Tro_c.txt = 142635 chars\n",
      "VenusAdonis_x.txt = 55527 chars\n",
      "Wiv_c.txt = 115202 chars\n",
      "WT_c.txt = 134528 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So shaken as we are , so wan with care , Find we a time for frighted peace to pant And breathe short-winded accents of new broils To be commenced in strands afar remote \n"
     ]
    }
   ],
   "source": [
    "# Check to see that the first sentence is correct\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 15:32:27,045 : INFO : collecting all words and their counts\n",
      "2019-05-07 15:32:27,046 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-05-07 15:32:27,434 : INFO : PROGRESS: at sentence #10000, processed 205010 words and 101389 word types\n",
      "2019-05-07 15:32:27,798 : INFO : PROGRESS: at sentence #20000, processed 403566 words and 170975 word types\n",
      "2019-05-07 15:32:28,172 : INFO : PROGRESS: at sentence #30000, processed 599588 words and 233068 word types\n",
      "2019-05-07 15:32:28,555 : INFO : PROGRESS: at sentence #40000, processed 798141 words and 286447 word types\n",
      "2019-05-07 15:32:28,943 : INFO : PROGRESS: at sentence #50000, processed 998259 words and 337166 word types\n",
      "2019-05-07 15:32:29,173 : INFO : collected 365018 word types from a corpus of 1107953 words (unigram + bigrams) and 55651 sentences\n",
      "2019-05-07 15:32:29,174 : INFO : using 365018 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-05-07 15:32:29,175 : INFO : source_vocab length 365018\n",
      "2019-05-07 15:32:32,959 : INFO : Phraser built with 1137 phrasegrams\n",
      "2019-05-07 15:32:32,969 : INFO : collecting all words and their counts\n",
      "2019-05-07 15:32:32,970 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-05-07 15:32:33,939 : INFO : PROGRESS: at sentence #10000, processed 177937 words and 100651 word types\n",
      "2019-05-07 15:32:34,857 : INFO : PROGRESS: at sentence #20000, processed 350165 words and 171156 word types\n",
      "2019-05-07 15:32:35,721 : INFO : PROGRESS: at sentence #30000, processed 520407 words and 233881 word types\n",
      "2019-05-07 15:32:36,565 : INFO : PROGRESS: at sentence #40000, processed 692661 words and 288512 word types\n",
      "2019-05-07 15:32:37,426 : INFO : PROGRESS: at sentence #50000, processed 866443 words and 340596 word types\n",
      "2019-05-07 15:32:37,911 : INFO : collected 369042 word types from a corpus of 961298 words (unigram + bigrams) and 55651 sentences\n",
      "2019-05-07 15:32:37,912 : INFO : using 369042 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-05-07 15:32:37,913 : INFO : source_vocab length 369042\n",
      "2019-05-07 15:32:42,223 : INFO : Phraser built with 2603 phrasegrams\n",
      "2019-05-07 15:32:52,112 : INFO : collecting all words and their counts\n",
      "2019-05-07 15:32:52,113 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-07 15:32:52,160 : INFO : PROGRESS: at sentence #10000, processed 175783 words, keeping 14778 word types\n",
      "2019-05-07 15:32:52,207 : INFO : PROGRESS: at sentence #20000, processed 346055 words, keeping 21104 word types\n",
      "2019-05-07 15:32:52,255 : INFO : PROGRESS: at sentence #30000, processed 514489 words, keeping 26209 word types\n",
      "2019-05-07 15:32:52,305 : INFO : PROGRESS: at sentence #40000, processed 684888 words, keeping 29786 word types\n",
      "2019-05-07 15:32:52,347 : INFO : PROGRESS: at sentence #50000, processed 857013 words, keeping 32922 word types\n",
      "2019-05-07 15:32:52,372 : INFO : collected 34965 word types from a corpus of 950847 raw words and 55651 sentences\n",
      "2019-05-07 15:32:52,373 : INFO : Loading a fresh vocabulary\n",
      "2019-05-07 15:32:52,443 : INFO : effective_min_count=1 retains 34965 unique words (100% of original 34965, drops 0)\n",
      "2019-05-07 15:32:52,444 : INFO : effective_min_count=1 leaves 950847 word corpus (100% of original 950847, drops 0)\n",
      "2019-05-07 15:32:52,545 : INFO : deleting the raw counts dictionary of 34965 items\n",
      "2019-05-07 15:32:52,546 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2019-05-07 15:32:52,547 : INFO : downsampling leaves estimated 711826 word corpus (74.9% of prior 950847)\n",
      "2019-05-07 15:32:52,656 : INFO : estimated required memory for 34965 words and 200 dimensions: 73426500 bytes\n",
      "2019-05-07 15:32:52,657 : INFO : resetting layer weights\n",
      "2019-05-07 15:32:53,108 : INFO : training model with 20 workers on 34965 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-07 15:32:54,124 : INFO : EPOCH 1 - PROGRESS: at 75.62% examples, 537527 words/s, in_qsize 24, out_qsize 0\n",
      "2019-05-07 15:32:54,162 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-07 15:32:54,167 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-07 15:32:54,168 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-07 15:32:54,195 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-07 15:32:54,196 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-07 15:32:54,205 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-07 15:32:54,213 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-07 15:32:54,214 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-07 15:32:54,222 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-07 15:32:54,224 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-07 15:32:54,226 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-07 15:32:54,240 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-07 15:32:54,249 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-07 15:32:54,255 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-07 15:32:54,260 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-07 15:32:54,260 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-07 15:32:54,262 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-07 15:32:54,276 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-07 15:32:54,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-07 15:32:54,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-07 15:32:54,278 : INFO : EPOCH - 1 : training on 950847 raw words (711284 effective words) took 1.2s, 615860 effective words/s\n",
      "2019-05-07 15:32:55,327 : INFO : EPOCH 2 - PROGRESS: at 76.18% examples, 528764 words/s, in_qsize 23, out_qsize 0\n",
      "2019-05-07 15:32:55,347 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-07 15:32:55,361 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-07 15:32:55,373 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-07 15:32:55,377 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-07 15:32:55,393 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-07 15:32:55,410 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-07 15:32:55,423 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-07 15:32:55,427 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-07 15:32:55,432 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-07 15:32:55,433 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-07 15:32:55,441 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-07 15:32:55,446 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-07 15:32:55,448 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-07 15:32:55,454 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-07 15:32:55,455 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-07 15:32:55,458 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-07 15:32:55,460 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-07 15:32:55,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-07 15:32:55,475 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-07 15:32:55,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-07 15:32:55,480 : INFO : EPOCH - 2 : training on 950847 raw words (711963 effective words) took 1.2s, 600531 effective words/s\n",
      "2019-05-07 15:32:56,502 : INFO : EPOCH 3 - PROGRESS: at 72.28% examples, 511616 words/s, in_qsize 26, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 15:32:56,574 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-07 15:32:56,590 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-07 15:32:56,594 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-07 15:32:56,599 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-07 15:32:56,608 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-07 15:32:56,638 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-07 15:32:56,645 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-07 15:32:56,652 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-07 15:32:56,661 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-07 15:32:56,672 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-07 15:32:56,677 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-07 15:32:56,682 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-07 15:32:56,685 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-07 15:32:56,686 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-07 15:32:56,687 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-07 15:32:56,688 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-07 15:32:56,694 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-07 15:32:56,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-07 15:32:56,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-07 15:32:56,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-07 15:32:56,713 : INFO : EPOCH - 3 : training on 950847 raw words (711714 effective words) took 1.2s, 583626 effective words/s\n",
      "2019-05-07 15:32:57,767 : INFO : EPOCH 4 - PROGRESS: at 75.62% examples, 517002 words/s, in_qsize 24, out_qsize 0\n",
      "2019-05-07 15:32:57,849 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-07 15:32:57,852 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-07 15:32:57,856 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-07 15:32:57,859 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-07 15:32:57,861 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-07 15:32:57,863 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-07 15:32:57,865 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-07 15:32:57,867 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-07 15:32:57,874 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-07 15:32:57,880 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-07 15:32:57,886 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-07 15:32:57,895 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-07 15:32:57,901 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-07 15:32:57,910 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-07 15:32:57,914 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-07 15:32:57,915 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-07 15:32:57,916 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-07 15:32:57,936 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-07 15:32:57,941 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-07 15:32:57,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-07 15:32:57,945 : INFO : EPOCH - 4 : training on 950847 raw words (711742 effective words) took 1.2s, 584436 effective words/s\n",
      "2019-05-07 15:32:58,987 : INFO : EPOCH 5 - PROGRESS: at 73.43% examples, 508766 words/s, in_qsize 24, out_qsize 2\n",
      "2019-05-07 15:32:59,051 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-07 15:32:59,059 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-07 15:32:59,066 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-07 15:32:59,070 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-07 15:32:59,082 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-07 15:32:59,099 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-07 15:32:59,100 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-07 15:32:59,106 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-07 15:32:59,107 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-07 15:32:59,110 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-07 15:32:59,125 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-07 15:32:59,132 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-07 15:32:59,135 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-07 15:32:59,140 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-07 15:32:59,150 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-07 15:32:59,151 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-07 15:32:59,158 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-07 15:32:59,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-07 15:32:59,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-07 15:32:59,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-07 15:32:59,174 : INFO : EPOCH - 5 : training on 950847 raw words (711816 effective words) took 1.2s, 585373 effective words/s\n",
      "2019-05-07 15:32:59,174 : INFO : training on a 4754235 raw words (3558519 effective words) took 6.1s, 586937 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'shaken', 'as', 'we_are', ',', 'so', 'wan', 'with', 'care', 'Find']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "#bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')\n",
    "#trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')\n",
    "#bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ', threshold=2)\n",
    "#trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ') #, threshold=3\n",
    "\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg=1)\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34965\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-07 15:36:04,034 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Edward', 0.8534423112869263),\n",
       " ('duke', 0.8479089140892029),\n",
       " ('prince', 0.8452507257461548),\n",
       " ('brother', 0.8438284397125244),\n",
       " ('Duke_of_York', 0.8431627750396729),\n",
       " ('queen', 0.8423248529434204),\n",
       " ('Richard', 0.8406826257705688),\n",
       " ('Montague', 0.8366484642028809),\n",
       " ('son', 0.8325438499450684),\n",
       " ('kinsman', 0.8297189474105835)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"king\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.9180877208709717),\n",
       " ('Cassio', 0.9079176187515259),\n",
       " ('mistress', 0.900288462638855),\n",
       " ('friar', 0.8967807292938232),\n",
       " ('widow', 0.8943183422088623),\n",
       " ('coz', 0.8941295146942139),\n",
       " ('Iago', 0.8928736448287964),\n",
       " ('Katherine', 0.8927853107452393),\n",
       " ('Arcite', 0.8921384215354919),\n",
       " ('knight', 0.8890625238418579)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"lady\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.8399614691734314),\n",
       " ('losing', 0.8385425806045532),\n",
       " ('revenge', 0.8370698690414429),\n",
       " ('conquest', 0.832743763923645),\n",
       " ('pain', 0.8327351808547974),\n",
       " ('Henry’s', 0.8295367956161499),\n",
       " ('wrath', 0.8293946981430054),\n",
       " ('eternal', 0.8281984329223633),\n",
       " ('doom', 0.8279428482055664),\n",
       " ('fortune', 0.818524181842804)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"death\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09687040542889289\n",
      "0.06743573433363965\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.relative_cosine_similarity(\"lady\",\"lord\",topn=10))\n",
    "print(model.wv.relative_cosine_similarity(\"lady\",\"meal\",topn=10))\n",
    "# From the gensim documentation: \"For WordNet synonyms, if rcs(topn=10) is greater than 0.10 \n",
    "# then wa and wb are more similar than any arbitrary word pairs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sighèd', 0.7990937232971191),\n",
       " ('herself', 0.7839317917823792),\n",
       " ('lily', 0.7815945148468018),\n",
       " ('feeds', 0.774307370185852),\n",
       " ('cavils', 0.7739996910095215),\n",
       " ('personage', 0.7718884944915771),\n",
       " ('At_last', 0.7706074714660645),\n",
       " ('listeth', 0.7651330232620239),\n",
       " ('Charges', 0.7633577585220337),\n",
       " ('Sing', 0.7630754709243774)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogies -- this example asks:\n",
    "# \"she\" is to \"sings\" as \"he\" is to ...   (analogies are often written like this: \"she:sings::he:?\")\n",
    "model.wv.most_similar(positive=['she','sings'],negative=['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.0004478463),\n",
       " ('would', 0.0004237212),\n",
       " ('know', 0.0004224492),\n",
       " ('believe', 0.00037739635),\n",
       " ('think', 0.0003541183),\n",
       " ('If', 0.0003515605),\n",
       " ('thee', 0.0003474701),\n",
       " ('confess', 0.00032659344),\n",
       " ('can', 0.00032383786),\n",
       " ('myself', 0.00031887807),\n",
       " ('do', 0.00031645462),\n",
       " ('hate', 0.0003124694),\n",
       " ('if', 0.00030926746),\n",
       " ('wrong', 0.0002904788),\n",
       " ('what', 0.00028437498),\n",
       " ('cannot', 0.00027943458),\n",
       " ('should', 0.00026911244),\n",
       " ('As', 0.0002673653),\n",
       " ('though', 0.00025443867),\n",
       " ('see', 0.00025254174),\n",
       " ('wish', 0.00025188673),\n",
       " ('How', 0.00024439913),\n",
       " ('What', 0.00023914642),\n",
       " ('may', 0.00023835502),\n",
       " ('her', 0.0002347324),\n",
       " ('feel', 0.0002323044),\n",
       " ('say', 0.0002317712),\n",
       " ('Do', 0.00022888358),\n",
       " ('well', 0.00022854333),\n",
       " ('will', 0.00022806987)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word([\"I\",\"do\",\"love\"], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
